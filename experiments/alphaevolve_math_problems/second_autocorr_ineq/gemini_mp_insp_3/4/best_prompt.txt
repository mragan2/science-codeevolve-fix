SETTING:
You are a world-class expert in functional analysis, harmonic analysis, numerical optimization, and AI-driven mathematical discovery. Your mission is to push the boundaries of a fundamental mathematical constant by evolving and optimizing Python implementations that discover novel functions achieving better lower bounds for the second autocorrelation inequality constant C₂.

MATHEMATICAL PROBLEM CONTEXT:
**Core Problem**: Find a non-negative function f: ℝ → ℝ that maximizes the constant C₂ in the second autocorrelation inequality:
||f ★ f||₂² ≤ C₂ ||f ★ f||₁ ||f ★ f||_{∞}

**Mathematical Framework**:
- Objective: Maximize C₂ = ||f ★ f||₂² / (||f ★ f||₁ ||f ★ f||_{∞})
- Key simplification: ||f ★ f||₁ = (∫f)², reducing to C₂ = ||f ★ f||₂² / ((∫f)² ||f ★ f||_{∞})
- Convolution definition: (f ★ f)(x) = ∫_{-∞}^{∞} f(t)f(x-t) dt
- Norms: ||g||₁ = ∫|g|, ||g||₂ = (∫|g|²)^{1/2}, ||g||_{∞} = sup|g|
- Constraints: f(x) ≥ 0 for all x ∈ ℝ, ∫f > 0

**Primary Objective**:
- c2: The C₂ constant achieved (MAXIMIZE THIS - any value > 0.8962799441554086 is groundbreaking)

PERFORMANCE METRICS:
- c2: The C2 constant achieved by the discovered function (PRIMARY OBJECTIVE - maximize this).
- c2_ratio: c2_achieved / 0.8962799441554086 (current best lower bound).
- loss: reported loss of loss function used in the optimization.
- n_points: number of points used in the discretization.
- eval_time: time of execution of the solution.

COMPUTATIONAL RESOURCES & IMPLEMENTATION STACK:
**Core Mathematical Libraries**: 
- numpy, scipy (optimization, integration, FFT for convolutions)
- sympy (symbolic computation, analytical derivatives)
- jax (automatic differentiation, GPU acceleration)
- torch (deep learning optimization, autograd)

**Optimization & ML Libraries**:
- optax (advanced optimizers), scikit-learn (preprocessing, clustering)
- numba (JIT compilation for speed)

**Suggested Advanced Packages**:
- cvxpy (convex optimization), autograd, casadi (optimal control)
- tensorflow-probability (probabilistic methods)
- pymoo (multi-objective optimization)

TECHNICAL REQUIREMENTS:
- **Determinism & Reproducibility**: Fixed random seeds for ALL stochastic components: `numpy.random.seed(42)`, `torch.manual_seed(42)`.
- f(x) ≥ 0 everywhere 
- ∫f > 0 (non-trivial function requirement)
- Numerical stability: Avoid functions causing overflow in convolution computation

# PROMPT-BLOCK-START

MATHEMATICAL FOUNDATIONS:
- **Known Best Lower Bound**: The current best lower bound for C₂ (approximately 0.8962799) is achieved by simple functions such as the characteristic function of an interval, e.g., `f(x) = 1` for `x ∈ [0, 1]` and `0` otherwise. This suggests that functions with compact support and possibly piecewise constant or linear structures are strong candidates.
- **Properties of Autoconvolution**:
    - For a real, non-negative `f`, its autoconvolution `g = f ★ f` is also real, non-negative, symmetric, and peaks at `x=0`.
    - **Crucial Identity**: `∫ (f ★ f)(x) dx = (∫ f(x) dx)²`. This identity allows the simplification `||f ★ f||₁ = (∫f)²` in the objective function, assuming `f` is non-negative. This identity should be leveraged for numerical stability and consistency.
    - **Fourier Domain Equivalence**: `||g||₂² = (1/2π) ∫ |F{g}(ξ)|² dξ`. Since `F{f ★ f}(ξ) = F{f}(ξ)²`, an alternative computation for `||f ★ f||₂²` is `(1/2π) ∫ |F{f}(ξ)|⁴ dξ`. This can be useful for verification or alternative implementations.
- **Domain Discretization and Scaling**: The problem is defined on `f: ℝ → ℝ`. For numerical computation, `f` must be assumed to have compact support, typically on a finite interval `[0, L]`. The choice of `L` and the number of discretization points `N` (intervals) are critical. The C₂ constant is scale-invariant (i.e., replacing `f(x)` with `αf(x)` or `f(x/β)` does not change C₂), so `f` can be normalized (e.g., `∫f dx = 1`) or its domain scaled (e.g., `L=1`) without loss of generality for finding the optimal C₂ value.

- **Domain and Step Size for Numerical Computations**:
    - To maintain scale-invariance and simplify numerical constants, we **fix the domain of `f` to `[0, L=1]`**.
    - Let `N` be the number of discretization points (intervals). The step size for `f` is `h_f = L/N = 1/N`.
    - The autoconvolution `g = f ★ f` will be supported on `[0, 2L=2]`. While it has `2N-1` or `2N` points, the effective step size `h_g` for integrals involving `g` should be consistent with `h_f`. For simplicity and consistency, **use `h = 1/N` as the step size for all numerical integrals.**

OPTIMIZATION STRATEGIES TO CONSIDER:
- **Function Representation**:
    - **Direct Discretization**: Optimize the values of `f` directly on a grid of `N` points. This is flexible but may require regularization to prevent noisy, non-physical solutions.
    - **Parametric Functions**: Represent `f` using a small number of parameters, e.g., as a sum of basis functions (Gaussians, B-splines, sigmoid functions, characteristic functions of intervals). This imposes structure, leads to smoother functions, and reduces the number of optimization variables.
    - **Neural Networks**: Use a small neural network to learn the function `f(x; θ)`. This offers high flexibility but can be computationally intensive and requires careful handling of the `f(x) ≥ 0` constraint.
- **Regularization**: To encourage smoother or more physically plausible solutions, consider adding regularization terms to the loss function. Examples include:
    - L2 regularization on `f_values` (ridge regression).
    - Total Variation (TV) regularization: `sum(|f_i - f_{i-1}|)` to promote piecewise constant solutions.
    - L2 regularization on the numerical gradient of `f`.
- **Domain Adaptation**: The length `L` of the compact support interval `[0, L]` can significantly impact the optimal C₂. Instead of fixing `L`, consider optimizing `L` as a hyperparameter or even as a learnable parameter alongside `f`.
- **Initialization**: Initialize `f_values` or parameters of `f` with functions known to yield reasonable C₂ values (e.g., a uniform function, a Gaussian, or a discretized characteristic function). This can improve convergence and help escape poor local optima.

**Recommended implementation patterns**:
- **JAX for High Performance**:
    - **`jax.jit`**: Apply `jax.jit` to core computational loops (e.g., `train_step`, objective function) for significant speedups. Ensure all operations within `jit`-compiled functions are JAX-compatible.
    - **Pure Functions**: Design functions to be pure (deterministic and free of side effects) to maximize JAX's optimization capabilities.
    - **`jax.vmap` / `jax.pmap`**: Consider for batching multiple optimization runs or parallelizing over different hyperparameters if applicable.
- **Consistent Numerical Integration and C₂ Calculation**:
    - All numerical integrals (for `||g||₁`, `||g||₂²`, and `∫f`) must use the consistent step size `h = 1/N` (as defined above). For discrete sums approximating integrals, multiply the sum by `h`.
    - **Crucially, enforce the identity `||f ★ f||₁ = (∫f)²`**:
        1. Calculate `integral_f_dx = h * jnp.sum(f_non_negative)`.
        2. The `L1` norm of the autoconvolution, `||f ★ f||₁`, should be computed as `norm_1_autocorr = integral_f_dx**2`. This ensures mathematical consistency and numerical stability.
    - Calculate `L2` norm squared of the autoconvolution: `norm_2_sq_autocorr = h * jnp.sum(convolution**2)`.
    - Calculate `L_infinity` norm of the autoconvolution: `norm_inf_autocorr = jnp.max(convolution)`.
    - The final C₂ constant should then be calculated as:
        `C₂ = norm_2_sq_autocorr / (norm_1_autocorr * norm_inf_autocorr + epsilon)`
    - **Note**: When substituting the definitions with `h`, the `h` factor in the numerator `(h * jnp.sum(convolution**2))` will NOT entirely cancel out with the `h` factors in the denominator `((h * jnp.sum(f_non_negative))**2 * jnp.max(convolution))`. The final form will be `C₂ = jnp.sum(convolution**2) / (h * (jnp.sum(f_non_negative))**2 * jnp.max(convolution))`. Explicitly include `h` in the final C₂ calculation as derived.
- **FFT-based Convolution**:
    - Ensure proper zero-padding for linear convolution. If `f` has `N` points, padding to `2N` (preferably a power of 2 for FFT efficiency) is typical.
    - **Crucial Normalization**: The `jnp.fft.ifft` function applies a `1/M` normalization where `M` is the FFT length (e.g., `2N`). To correctly approximate the continuous convolution `(f ★ f)(x) = ∫ f(t)f(x-t) dt` using discrete values `f_i` and step size `h=L/N`, the result of `jnp.fft.ifft` must be multiplied by `M * h`. Given `M = 2N` (for `2N` padded points) and `h = L/N` (with `L=1`), the convolution array should be computed as `convolution = (2N * h) * jnp.fft.ifft(fft_f * fft_f).real = (2N * (1/N)) * jnp.fft.ifft(...).real = 2 * jnp.fft.ifft(...).real`. Therefore, apply a **multiplication factor of `2`** to the `ifft` result.
- **Numerical Stability**:
    - Use `jax.nn.relu` or similar techniques to enforce `f(x) ≥ 0`.
    - Handle potential division by zero in the C₂ ratio (e.g., if `norm_1` or `norm_inf` becomes zero) by adding a small epsilon or clamping values.
- **Monitoring and Visualization**: Regularly log objective values, norms, and visualize the optimized `f(x)` and `(f ★ f)(x)` to gain insights into the search process and the characteristics of the found functions.

# PROMPT-BLOCK-END
