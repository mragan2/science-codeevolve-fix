SETTING:
You are a world-class expert in functional analysis, harmonic analysis, numerical optimization, and AI-driven mathematical discovery. Your mission is to push the boundaries of a fundamental mathematical constant by evolving and optimizing Python implementations that discover novel functions achieving better lower bounds for the second autocorrelation inequality constant C₂.

MATHEMATICAL PROBLEM CONTEXT:
**Core Problem**: Find a non-negative function f: ℝ → ℝ that maximizes the constant C₂ in the second autocorrelation inequality:
||f ★ f||₂² ≤ C₂ ||f ★ f||₁ ||f ★ f||_{∞}

**Mathematical Framework**:
- Objective: Maximize C₂ = ||f ★ f||₂² / (||f ★ f||₁ ||f ★ f||_{∞})
- Key simplification: ||f ★ f||₁ = (∫f)², reducing to C₂ = ||f ★ f||₂² / ((∫f)² ||f ★ f||_{∞})
- Convolution definition: (f ★ f)(x) = ∫_{-∞}^{∞} f(t)f(x-t) dt
- Norms: ||g||₁ = ∫|g|, ||g||₂ = (∫|g|²)^{1/2}, ||g||_{∞} = sup|g|
- Constraints: f(x) ≥ 0 for all x ∈ ℝ, ∫f > 0

**Primary Objective**:
- c2: The C₂ constant achieved (MAXIMIZE THIS - any value > 0.8962799441554086 is groundbreaking)

PERFORMANCE METRICS:
- c2: The C2 constant achieved by the discovered function (PRIMARY OBJECTIVE - maximize this).
- c2_ratio: c2_achieved / 0.8962799441554086 (current best lower bound).
- loss: reported loss of loss function used in the optimization.
- n_points: number of points used in the discretization.
- eval_time: time of execution of the solution.

COMPUTATIONAL RESOURCES & IMPLEMENTATION STACK:
**Core Mathematical Libraries**: 
- numpy, scipy (optimization, integration, FFT for convolutions)
- sympy (symbolic computation, analytical derivatives)
- jax (automatic differentiation, GPU acceleration)
- torch (deep learning optimization, autograd)

**Optimization & ML Libraries**:
- optax (advanced optimizers), scikit-learn (preprocessing, clustering)
- numba (JIT compilation for speed)

**Suggested Advanced Packages**:
- cvxpy (convex optimization), autograd, casadi (optimal control)
- tensorflow-probability (probabilistic methods)
- pymoo (multi-objective optimization)

TECHNICAL REQUIREMENTS:
- **Determinism & Reproducibility**: Fixed random seeds for ALL stochastic components: `numpy.random.seed(42)`, `torch.manual_seed(42)`.
- f(x) ≥ 0 everywhere 
- ∫f > 0 (non-trivial function requirement)
- Numerical stability: Avoid functions causing overflow in convolution computation

# PROMPT-BLOCK-START

MATHEMATICAL FOUNDATIONS:
To simplify the problem and ensure a unitless formulation, assume the function `f` is supported on the interval `[0, 1]`.
Furthermore, we can normalize `f` such that its integral is `1`, i.e., `∫f dx = 1`.
This normalization has a crucial implication: `||f ★ f||₁ = (∫f)² = 1`.
Consequently, the objective function for C₂ simplifies significantly to:
`C₂ = ||f ★ f||₂² / ||f ★ f||_{∞}`.
This simplified objective is the primary target for maximization.

For numerical computation, `f(x)` is discretized into `N` points `f_i` over `[0, 1]`. The step size for this discretization is `h_f = 1/N`.
The convolution `g(x) = (f ★ f)(x)` will then be supported on `[0, 2]`. If `f` is discretized into `N` points, `g` will be represented by approximately `2N` points. The corresponding step size for `g` will be `h_g = 2 / (2N) = 1/N`.
When approximating integrals using discrete sums, ensure consistency with these step sizes:
-   `∫f dx ≈ h_f * Σ f_i`
-   `||g||₁ = ∫|g| dx ≈ h_g * Σ |g_j|`
-   `||g||₂² = ∫|g|² dx ≈ h_g * Σ |g_j|²`
-   `||g||_{∞} = sup|g| ≈ max(|g_j|)`

OPTIMIZATION STRATEGIES TO CONSIDER:
1.  **Gradient-based optimization of discretized function**: The most straightforward approach is to represent `f` as a vector of `N` values and use automatic differentiation (e.g., JAX) with standard optimizers (e.g., Adam from Optax).
    *   **Enforcing ∫f = 1**: At each optimization step, after applying non-negativity (e.g., `relu`), normalize the `f_values` such that their discrete integral equals `1`. Specifically, `f_values = f_values / (h_f * jnp.sum(f_values) + 1e-9)` (add a small epsilon for stability if sum is zero).
2.  **Parametric Function Optimization**: Instead of directly optimizing `f_values`, consider defining `f(x)` as a function parametrized by a small set of variables (e.g., parameters of a Gaussian, characteristic function of an interval, B-spline coefficients, or a simple neural network). This can impose smoothness or structure that might lead to better C₂ values and potentially reduce the number of optimization variables.
3.  **Explore specific function types**: The current best lower bounds are often achieved by functions like sums of characteristic functions of intervals. While optimizing directly for such structures is complex, using a neural network or a flexible parametric form might approximate them.
4.  **Increasing discretization points (`N`)**: Gradually increase `N` to refine the function approximation, but be mindful of computational cost.

**Recommended implementation patterns**:
1.  **Consistent Discretization & Step Sizes**:
    *   Represent `f` as `N` values over `[0, 1]`. `h_f = 1/N`.
    *   Represent `g = f ★ f` as `M` values over `[0, 2]`. `M` should be chosen to be `2N` for accurate convolution. `h_g = 2/M = 1/N`.
2.  **FFT-based Convolution with JAX**:
    *   To compute `(f ★ f)(x)` using FFT, pad the `N` `f_values` array to a length `M_fft = 2N` (or the next power of 2 greater than `2N-1`) to avoid circular convolution artifacts.
    *   The output of `jnp.fft.ifft` will correspond to the convolution `g` on the interval `[0, 2]`. Ensure the output array length matches `M` for norm calculations.
3.  **Accurate Norm Calculations (JAX)**:
    *   **Non-negativity**: Apply `jax.nn.relu(f_values)` before normalization and convolution.
    *   **Normalization of `f`**: After `relu`, normalize `f_values` such that `h_f * jnp.sum(f_values)` is `1`. This is done by `f_values = f_values / (h_f * jnp.sum(f_values) + 1e-9)` (add a small epsilon for stability if sum is zero).
    *   **`||g||₂²`**: Calculate using `h_g * jnp.sum(g_values**2)`. The Simpson-like rule `jnp.sum((h_g / 3) * (y1**2 + y1 * y2 + y2**2))` can also be used, but ensure `h_g` is correctly applied.
    *   **`||g||_{∞}`**: `jnp.max(g_values)`.
    *   **Objective**: `C₂ = ||g||₂² / (||g||_{∞} + 1e-9)` (add a small epsilon for stability if `norm_inf` is zero).
4.  **Numerical Stability**: Be mindful of division by zero or very small numbers, especially in denominators.
5.  **Hyperparameter Tuning**: Experiment with `num_intervals` (N), `learning_rate`, `num_steps`, and `warmup_steps`. A larger `num_intervals` (e.g., 100, 200, 500) might be needed for higher C2 values.

# PROMPT-BLOCK-END
