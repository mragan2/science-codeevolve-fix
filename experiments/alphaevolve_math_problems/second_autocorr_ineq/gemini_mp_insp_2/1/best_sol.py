# EVOLVE-BLOCK-START
import jax
import jax.numpy as jnp
import optax
import numpy as np
from dataclasses import dataclass


@dataclass
class Hyperparameters:
    """Hyperparameters for the optimization process."""

    num_intervals: int = 200 # Increased resolution for potentially better accuracy
    learning_rate: float = 0.001 # Reduced learning rate for stability with MLP
    num_steps: int = 20000 # Increased steps for MLP training
    warmup_steps: int = 2000 # Increased warmup steps
    mlp_hidden_dim: int = 64 # Hidden dimension for the MLP

# MLP definition
def init_mlp_params(key, input_dim, hidden_dim, output_dim):
    """Initializes MLP parameters using Glorot initialization."""
    params = {}
    # Layer 1
    key, subkey = jax.random.split(key)
    params['dense1'] = {
        'weights': jax.random.normal(subkey, (input_dim, hidden_dim)) * jnp.sqrt(2. / input_dim),
        'bias': jnp.zeros((hidden_dim,))
    }
    # Layer 2
    key, subkey = jax.random.split(key)
    params['dense2'] = {
        'weights': jax.random.normal(subkey, (hidden_dim, output_dim)) * jnp.sqrt(2. / hidden_dim),
        'bias': jnp.zeros((output_dim,))
    }
    return params

def mlp_apply(params, x):
    """Applies the MLP to input x."""
    # x is expected to be (num_points, input_dim)
    h = jnp.dot(x, params['dense1']['weights']) + params['dense1']['bias']
    h = jax.nn.tanh(h) # Tanh activation in hidden layer for smooth gradients
    output = jnp.dot(h, params['dense2']['weights']) + params['dense2']['bias']
    return output


class C2Optimizer:
    """
    Optimizes a discretized function to find a lower bound for the C2 constant
    using the problem's specified norm definitions and numerical integration.
    The function f(x) is represented by a small MLP.
    """

    def __init__(self, hypers: Hyperparameters):
        self.hypers = hypers
        self.mlp_init_key = jax.random.PRNGKey(42) # Key for MLP parameter initialization
        self.mlp_input_dim = 1 # MLP takes a scalar x as input
        self.mlp_output_dim = 1 # MLP outputs a scalar f(x)
        self.mlp_apply_fn = mlp_apply # Store the apply function

    def _objective_fn(self, params) -> jnp.ndarray: # Now optimizes MLP params
        """
        Computes the objective function (negative C2 ratio) based on the
        discretized function f_values generated by the MLP.
        """
        N = self.hypers.num_intervals
        dx_f = 1.0 / N # Step size for f on [0, 1]

        # Generate x_points on [0,1] and evaluate MLP to get f_values
        x_points = jnp.linspace(0.0, 1.0, N, endpoint=False).reshape(-1, self.mlp_input_dim)
        f_raw_values = self.mlp_apply_fn(params, x_points).flatten()

        # 1. Ensure f is non-negative and normalize such that ∫f(x) dx = 1
        f_non_negative = jax.nn.relu(f_raw_values)
        integral_f_current = jnp.sum(f_non_negative) * dx_f

        # Normalize f_values to ensure ∫f = 1. Guard against division by zero.
        f_normalized = jnp.where(
            integral_f_current > 1e-12, # Check if integral is non-trivial
            f_non_negative / integral_f_current,
            jnp.zeros_like(f_non_negative) # If trivial, f_normalized becomes all zeros
        )
        # Now, integral_f for f_normalized is effectively 1.0 (or very close to it)
        # Therefore, ||f ★ f||₁ = (∫f)² = 1.0

        # 2. Compute convolution (f * f) using f_normalized
        # The convolution of f on [0,1] with itself is on [0,2].
        # For N discrete points of f, the linear convolution will have 2N-1 points.
        # We pad f_normalized to 2N points for efficient FFT-based linear convolution.
        M = 2 * N
        padded_f = jnp.pad(f_normalized, (0, N)) # Pad N zeros at the end to make length 2N

        fft_f = jnp.fft.fft(padded_f)
        # JAX's jnp.fft.ifft scales by 1/M. We multiply by M to recover the true discrete convolution.
        convolution_discrete = jnp.fft.ifft(fft_f * fft_f).real * M

        # Step size for integration of the convolution, which is on domain [0, 2]
        dx_conv = 2.0 / M # Which simplifies to 1.0 / N

        # 3. Calculate Norms of convolution_discrete (g = f * f)
        # ||g||₁ = (∫f)² = 1.0 because f_normalized integrates to 1.
        norm_1_conv = 1.0

        # ||g||₂² = ∫ g(x)² dx
        l2_norm_squared_conv = jnp.sum(convolution_discrete**2) * dx_conv

        # ||g||_{∞} = sup |g(x)|
        norm_inf_conv = jnp.max(jnp.abs(convolution_discrete))

        # Define conditions for trivial solutions and apply penalties using jnp.where.
        is_trivial = jnp.logical_or(integral_f_current < 1e-12, norm_inf_conv < 1e-12)

        # Calculate denominator, guarding against tiny values.
        denominator_guarded = jnp.maximum(norm_1_conv * norm_inf_conv, 1e-12)

        # 4. Calculate C2 ratio
        c2_ratio = l2_norm_squared_conv / denominator_guarded

        # Clip c2_ratio to prevent extreme values from destabilizing the optimizer.
        # This helps maintain gradient stability during training.
        c2_ratio_clipped = jnp.clip(c2_ratio, a_min=0.0, a_max=10.0) # Heuristic max of 10.0

        # We want to MAXIMIZE C2, so the optimizer must MINIMIZE its negative.
        # If trivial, return a large penalty, otherwise return -c2_ratio_clipped.
        loss_val = jnp.where(is_trivial, jnp.array(1e12), -c2_ratio_clipped)

        return loss_val

    def train_step(self, params, opt_state: optax.OptState) -> tuple: # Now takes MLP params
        """Performs a single training step."""
        loss, grads = jax.value_and_grad(self._objective_fn)(params)
        updates, opt_state = self.optimizer.update(grads, opt_state, params)
        params = optax.apply_updates(params, updates)
        return params, opt_state, loss

    def run_optimization(self):
        """Sets up and runs the full optimization process."""
        np.random.seed(42) # For any numpy operations
        key = jax.random.PRNGKey(42) # Primary JAX PRNGKey for reproducibility

        # Initialize MLP parameters
        params = init_mlp_params(
            key, self.mlp_input_dim, self.hypers.mlp_hidden_dim, self.mlp_output_dim
        )
        
        # Configure learning rate schedule
        schedule = optax.warmup_cosine_decay_schedule(
            init_value=0.0,
            peak_value=self.hypers.learning_rate,
            warmup_steps=self.hypers.warmup_steps,
            decay_steps=self.hypers.num_steps - self.hypers.warmup_steps,
            end_value=self.hypers.learning_rate * 1e-4,
        )
        self.optimizer = optax.adam(learning_rate=schedule)

        opt_state = self.optimizer.init(params) # Initialize optimizer with MLP parameters
        print(
            f"Number of intervals (N): {self.hypers.num_intervals}, Steps: {self.hypers.num_steps}, Hidden Dim: {self.hypers.mlp_hidden_dim}"
        )
        train_step_jit = jax.jit(self.train_step)

        loss = jnp.inf
        for step in range(self.hypers.num_steps):
            params, opt_state, loss = train_step_jit(params, opt_state) # Optimize MLP params
            if step % 1000 == 0 or step == self.hypers.num_steps - 1:
                # Re-evaluate C2 from current parameters for accurate logging
                current_c2 = -self._objective_fn(params)
                print(f"Step {step:5d} | C2 ≈ {current_c2:.8f}")

        # Re-evaluate final C2 with the optimized MLP parameters for a precise final metric
        final_c2 = -self._objective_fn(params)
        print(f"Final C2 lower bound found: {final_c2:.8f}")

        # Generate the final optimized f_values from the MLP for return
        x_points = jnp.linspace(0.0, 1.0, self.hypers.num_intervals, endpoint=False).reshape(-1, self.mlp_input_dim)
        final_f_raw_values = self.mlp_apply_fn(params, x_points).flatten()
        final_f_non_negative = jax.nn.relu(final_f_raw_values)
        
        # Ensure the returned f is also normalized
        dx_f = 1.0 / self.hypers.num_intervals
        final_integral_f = jnp.sum(final_f_non_negative) * dx_f
        optimized_f_normalized = jnp.where(
            final_integral_f > 1e-12,
            final_f_non_negative / final_integral_f,
            jnp.zeros_like(final_f_non_negative)
        )
        return optimized_f_normalized, final_c2


def run():
    """Entry point for running the optimization."""
    hypers = Hyperparameters()
    optimizer = C2Optimizer(hypers)
    optimized_f, final_c2_val = optimizer.run_optimization()

    # The reported loss is the negative of the final C2 value, as per optimization objective.
    loss_val = -final_c2_val 
    f_values_np = np.array(optimized_f)

    return f_values_np, float(final_c2_val), float(loss_val), hypers.num_intervals


# EVOLVE-BLOCK-END
